[34m[1mwandb[39m[22m: [33mWARNING[39m Calling wandb.run.save without any arguments is deprecated.Changes to attributes are automatically persisted.
  0%|                                                                                                                                                   | 0/500 [00:00<?, ?it/s]
tensor([[[ 8.4618e-01,  1.3827e+00, -2.1069e-01,  ..., -2.6946e+00,
           8.3131e-01,  2.2875e-01],
         [-1.1346e+00,  4.8564e-01,  2.3555e-01,  ...,  1.0292e+00,
           2.0878e-01,  4.5357e-01],
         [-8.3875e-01,  1.2402e+00,  6.4221e-01,  ...,  1.3789e+00,
           1.2809e+00, -1.5265e-01],
         ...,
         [-1.8598e+00,  2.9314e-01,  4.8386e-01,  ...,  8.9864e-01,
          -7.1349e-01,  4.3350e-01],
         [ 2.3643e-01, -1.1880e-01,  7.3606e-02,  ...,  1.1934e-01,
           1.4193e+00, -1.6068e-01],
         [-3.7776e-02,  3.4704e-01,  3.2552e-01,  ..., -4.6198e-01,
          -1.2939e-01, -2.6908e-01]],
        [[ 1.0407e+00,  5.9174e-01, -4.2180e-01,  ...,  3.4854e+00,
           3.5409e+00,  3.7665e+00],
         [-2.6490e+00, -2.5054e+00, -2.1147e+00,  ..., -6.2708e-01,
          -4.8028e-01,  1.0425e-01],
         [-1.4254e-02, -1.1631e-01,  4.2716e-01,  ...,  3.6206e-01,
          -5.7001e-02, -6.3040e-01],
         ...,
         [-7.5528e-01, -9.5681e-01, -6.0878e-01,  ..., -3.1160e+00,
          -3.0729e+00, -3.2928e+00],
         [-2.0366e+00, -2.4289e+00, -2.4482e+00,  ...,  1.0233e+00,
           1.3451e+00,  5.7377e-01],
         [ 4.2830e-01,  2.0461e-01, -4.5718e-01,  ..., -1.2292e+00,
          -1.1099e+00, -3.1555e-01]],
        [[-8.3678e-01, -1.3999e+00, -9.6780e-01,  ...,  6.5010e-01,
           1.4489e+00,  1.0139e+00],
         [ 5.4471e-01, -1.0356e-01,  4.6073e-01,  ..., -4.1053e-01,
          -4.7938e-01, -3.0969e-01],
         [-2.0984e+00, -1.5915e+00, -2.2616e-01,  ..., -5.2594e-01,
          -1.2158e+00, -7.8117e-01],
         ...,
         [-9.9028e-01,  4.9061e-01, -6.1903e-02,  ...,  9.4136e-01,
           5.7188e-01,  5.0264e-01],
         [-1.0862e+00, -9.8255e-01, -7.5875e-01,  ...,  1.3480e+00,
           1.9211e+00,  1.6388e+00],
         [ 2.4731e+00,  1.9883e+00,  2.7454e+00,  ...,  1.1447e+00,
           9.7102e-01,  1.1445e+00]],
        ...,
        [[-2.6940e-01, -2.5020e-01, -2.7908e-01,  ..., -1.1969e+00,
          -1.3276e-01,  3.4293e-02],
         [ 3.8659e-01, -2.1733e-01,  2.6059e-01,  ...,  1.0473e-01,
           4.1481e-01,  6.5700e-01],
         [-1.6145e-01, -4.4748e-01, -2.2926e-01,  ..., -1.0079e-01,
           6.4025e-02, -5.0071e-01],
         ...,
         [-1.5316e+00, -1.3270e+00, -1.1010e+00,  ...,  1.3899e-01,
          -1.0709e-01,  2.3602e-01],
         [-2.8601e-01, -2.1733e-01, -5.4907e-02,  ..., -1.0079e-01,
           3.6347e-01,  6.0605e-02],
         [ 1.2419e+00,  1.2951e+00,  1.1157e+00,  ..., -3.1488e-01,
           2.6936e-01, -7.8935e-04]],
        [[ 5.2720e-01,  5.1664e-01,  3.8575e-01,  ...,  7.2101e-01,
           6.1695e-01,  6.1491e-01],
         [ 1.0034e-02,  2.0520e-01, -1.7213e-02,  ..., -7.4106e-01,
          -8.0830e-01, -8.4773e-01],
         [-2.9748e-02, -2.5538e-01, -1.9679e-01,  ...,  6.7257e-01,
           4.2838e-01,  2.6687e-01],
         ...,
         [ 1.5394e+00,  2.0388e+00,  2.6327e+00,  ...,  6.2853e-01,
           5.0293e-01,  4.7394e-01],
         [ 9.5595e-01,  9.1581e-01,  9.0696e-01,  ..., -1.7451e+00,
          -1.6503e+00, -1.8830e+00],
         [-8.5190e-01, -5.0541e-01, -1.7051e-01,  ..., -8.8198e-01,
          -8.3900e-01, -8.8298e-01]],
        [[-1.2121e-01,  4.0528e-01,  3.2840e-01,  ...,  3.4753e-01,
           1.8229e-01,  4.0343e-01],
         [ 3.5802e-01,  4.4432e-01, -5.5645e-01,  ...,  6.0916e-01,
           7.1634e-01,  1.1737e+00],
         [ 2.7141e-01,  5.1591e-01,  1.1039e-01,  ...,  1.6499e-01,
          -1.3186e-01, -2.9567e-01],
         ...,
         [ 4.5040e-01,  1.3845e-01,  3.2840e-01,  ..., -1.2707e-01,
           4.4065e-02, -2.3800e-02],
         [ 8.5457e-01,  7.3719e-01,  9.1830e-01,  ..., -1.2527e+00,
          -1.2691e+00, -1.3314e+00],
         [-5.1961e-01, -9.0284e-01, -7.7446e-01,  ..., -1.0093e+00,
          -1.0869e+00, -1.5320e+00]]])
torch.Size([1424, 76, 8])
  0%|                                                                                                                                                   | 0/500 [00:04<?, ?it/s]
Traceback (most recent call last):
  File "/Users/shamba/Desktop/Paper 1 - HAR RL/NODATA_MarginMultistepCL/baselines/TimeDRL.py", line 433, in <module>
    main(train_loader, valid_loader, valid_balanced_dataloader, seed)
  File "/Users/shamba/Desktop/Paper 1 - HAR RL/NODATA_MarginMultistepCL/baselines/TimeDRL.py", line 209, in main
    ) = attn_model(x)
        ^^^^^^^^^^^^^
  File "/Users/shamba/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/shamba/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1561, in _call_impl
    result = forward_call(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/shamba/Desktop/Paper 1 - HAR RL/NODATA_MarginMultistepCL/baselines/src/models/timedrlmodel.py", line 158, in forward
    z_1 = self.encoder(x_1)  # (B * C, T_p + 1, D) or (B, T_p + 1, D)
          ^^^^^^^^^^^^^^^^^
  File "/Users/shamba/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/shamba/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/shamba/Desktop/Paper 1 - HAR RL/NODATA_MarginMultistepCL/baselines/src/models/attention_model.py", line 99, in forward
    x = self.conv1(x)  # Shape: (batch_size, 128, sequence_length)
        ^^^^^^^^^^^^^
  File "/Users/shamba/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/shamba/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/shamba/anaconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 310, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/shamba/anaconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 306, in _conv_forward
    return F.conv1d(input, weight, bias, self.stride,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Given groups=1, weight of size [128, 178, 1], expected input[1424, 32, 76] to have 178 channels, but got 32 channels instead